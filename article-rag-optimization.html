<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Optimizing RAG Pipelines - Cassie Liang</title>
    <style>
        :root {
            --bg-color: #ffffff;
            --text-color: #333333;
            --link-color: #000000;
            --meta-color: #666666;
            --font-main: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen-Sans, Ubuntu, Cantarell, "Helvetica Neue", sans-serif;
        }

        body {
            font-family: var(--font-main);
            line-height: 1.8;
            color: var(--text-color);
            background-color: var(--bg-color);
            max-width: 680px;
            margin: 0 auto;
            padding: 40px 20px;
        }

        header {
            margin-bottom: 40px;
            border-bottom: 1px solid #eee;
            padding-bottom: 20px;
        }

        .back-link {
            font-size: 14px;
            color: var(--meta-color);
            text-decoration: none;
            margin-bottom: 20px;
            display: inline-block;
        }

        h1 {
            font-size: 28px;
            font-weight: 700;
            margin: 0 0 10px 0;
            letter-spacing: -0.5px;
        }

        .date {
            color: var(--meta-color);
            font-size: 14px;
        }

        article {
            font-size: 16px;
        }

        h2 {
            font-size: 20px;
            margin-top: 30px;
            margin-bottom: 15px;
        }

        p {
            margin-bottom: 20px;
        }

        ul {
            margin-bottom: 20px;
        }

        li {
            margin-bottom: 10px;
        }

        code {
            background: #f4f4f4;
            padding: 2px 5px;
            border-radius: 3px;
            font-size: 14px;
        }
    </style>
</head>
<body>
    <header>
        <a href="index.html" class="back-link">‚Üê Back to Home</a>
        <h1>Optimizing RAG Pipelines: Reducing Latency to &lt;200ms</h1>
        <div class="date">December 20, 2025</div>
    </header>

    <article>
        <p>In modern AI applications, Retrieval-Augmented Generation (RAG) is crucial for providing up-to-date context to LLMs. However, latency is often a bottleneck. Here's how I optimized the <strong>Becoming</strong> platform to achieve sub-200ms responses.</p>

        <h2>1. Semantic Caching with pgvector</h2>
        <p>Instead of hitting the LLM for every query, I implemented a semantic cache layer. Incoming queries are embedded and compared against stored embeddings in PostgreSQL using <code>pgvector</code>.</p>
        <p>If a similar query (cosine similarity > 0.95) exists, the cached response is returned immediately, bypassing the expensive LLM generation step.</p>

        <h2>2. Optimistic UI Updates</h2>
        <p>Perception is reality. By implementing optimistic UI updates in React, the interface reflects the user's action immediately while the backend processes the request in the background. This reduced the <em>perceived</em> latency to near zero for task interactions.</p>

        <h2>3. Edge Functions</h2>
        <p>Moving the API logic to Supabase Edge Functions reduced the cold start time significantly compared to traditional serverless containers. The closer proximity to the user also shaved off vital milliseconds in network round-trips.</p>

        <h2>Results</h2>
        <ul>
            <li><strong>Average Latency:</strong> Reduced from 800ms to 180ms.</li>
            <li><strong>Cost Savings:</strong> 60% reduction in LLM API costs due to caching.</li>
            <li><strong>User Satisfaction:</strong> Improved significantly due to snappier UI interactions.</li>
        </ul>
    </article>
</body>
</html>